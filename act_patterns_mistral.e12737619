Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]Loading checkpoint shards:  33%|███▎      | 1/3 [00:08<00:16,  8.15s/it]Loading checkpoint shards:  67%|██████▋   | 2/3 [00:16<00:08,  8.14s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.76s/it]Loading checkpoint shards: 100%|██████████| 3/3 [00:23<00:00,  7.86s/it]
MistralModel is using MistralSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.
Traceback (most recent call last):
  File "/home/s2410121/proj_LA/activated_neuron/new_neurons/transfer_neurons/act_patterns/mistral.py", line 64, in <module>
    activation_patterns_lineplot(act_patterns, act_patterns_baseline, L2, None, model_type, "normal")
TypeError: activation_patterns_lineplot() missing 1 required positional argument: 'intervention'
