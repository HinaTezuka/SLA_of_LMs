result_main: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.865}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.986}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.993}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.712}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.8}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.693}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.561}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.703}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.832}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.935}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.968}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.774}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.932}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.926}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.789}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.912}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.912}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.735}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.621}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.5}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.781}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.943}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.744}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.955}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.542}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.824}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.713}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.703}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.634}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.975}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.788}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.865}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.815}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.569}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.845}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.519}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.604}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.701}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.941}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.703}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.734}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.714}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.765}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.884}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.995}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.761}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.608}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.476}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.907}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.87}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.99}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.659}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.501}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.911}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.839}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.733}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.782}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.729}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.793}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.835}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.903}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.886}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.957}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.957}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.356}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.279}]
result_shared_non_translation: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.797}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.499}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.496}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.607}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.484}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.456}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.558}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.407}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.414}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.58}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.56}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.51}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.627}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.544}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.462}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.599}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.544}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.516}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.474}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.419}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.31}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.826}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.59}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.611}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.289}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.547}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.559}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.403}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.375}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.876}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.39}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.554}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.542}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.801}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.353}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.132}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.714}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.747}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.204}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.806}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.468}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.528}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.759}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 0.983}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.436}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.58}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.586}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.384}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.471}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.545}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.445}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.521}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.346}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.558}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.232}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.324}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.744}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.536}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.008}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.527}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.76}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.547}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.762}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.639}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.177}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.3}]
result_comp: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.884}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.979}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.992}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.726}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.801}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.73}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.592}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.754}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.85}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.944}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.973}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.811}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.939}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.942}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.816}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.925}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.92}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.776}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.701}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.482}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.774}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.957}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.769}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.966}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.409}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.869}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.771}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.693}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.584}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.987}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.844}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.877}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.826}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.556}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.862}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.564}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.619}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.721}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.904}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.698}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.739}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.711}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.788}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.893}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.998}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.827}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.628}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.431}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.92}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.868}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.998}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.653}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.445}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.91}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.865}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.754}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.802}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.745}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.751}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.833}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.927}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.868}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.962}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.957}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.304}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.271}]
result_comp_L1_or_L2: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.86}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.988}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.99}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.724}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.768}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.721}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.633}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.756}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.847}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.938}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.971}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.789}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.918}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.916}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.794}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.897}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.917}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.68}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.604}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.51}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.781}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.952}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.7}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.958}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.22}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.818}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.721}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.692}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.613}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.988}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.807}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.841}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.707}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.674}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.863}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.415}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.616}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.717}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.643}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.503}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.74}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.743}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.776}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.901}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.998}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.837}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.646}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.45}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.89}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.814}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.988}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.608}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.409}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.8}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.831}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.757}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.789}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.745}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.595}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.848}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.918}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.876}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.966}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.957}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.307}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.279}]
result_comp_L1_specific: [{'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'adjunct_island', 'Accuracy': 0.893}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_gender_agreement', 'Accuracy': 0.989}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'anaphor_number_agreement', 'Accuracy': 0.992}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_passive', 'Accuracy': 0.72}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'animate_subject_trans', 'Accuracy': 0.738}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'causative', 'Accuracy': 0.71}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'complex_NP_island', 'Accuracy': 0.648}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_complex_left_branch', 'Accuracy': 0.786}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'coordinate_structure_constraint_object_extraction', 'Accuracy': 0.822}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_1', 'Accuracy': 0.94}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_2', 'Accuracy': 0.972}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_1', 'Accuracy': 0.795}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_irregular_2', 'Accuracy': 0.912}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_2', 'Accuracy': 0.902}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_1', 'Accuracy': 0.791}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adj_irregular_2', 'Accuracy': 0.883}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'determiner_noun_agreement_with_adjective_1', 'Accuracy': 0.923}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relational_noun', 'Accuracy': 0.68}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'distractor_agreement_relative_clause', 'Accuracy': 0.619}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'drop_argument', 'Accuracy': 0.506}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_1', 'Accuracy': 0.777}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'ellipsis_n_bar_2', 'Accuracy': 0.957}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_object_raising', 'Accuracy': 0.691}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_1', 'Accuracy': 0.958}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_quantifiers_2', 'Accuracy': 0.249}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'existential_there_subject_raising', 'Accuracy': 0.801}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'expletive_it_object_raising', 'Accuracy': 0.705}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'inchoative', 'Accuracy': 0.693}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'intransitive', 'Accuracy': 0.622}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_adjectives', 'Accuracy': 0.978}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_past_participle_verbs', 'Accuracy': 0.839}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_1', 'Accuracy': 0.836}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'irregular_plural_subject_verb_agreement_2', 'Accuracy': 0.765}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_echo_question', 'Accuracy': 0.676}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'left_branch_island_simple_question', 'Accuracy': 0.879}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'matrix_question_npi_licensor_present', 'Accuracy': 0.4}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_1', 'Accuracy': 0.617}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'npi_present_2', 'Accuracy': 0.687}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_licensor_present', 'Accuracy': 0.718}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'only_npi_scope', 'Accuracy': 0.523}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_1', 'Accuracy': 0.722}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'passive_2', 'Accuracy': 0.726}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_c_command', 'Accuracy': 0.762}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_1', 'Accuracy': 1.0}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_case_2', 'Accuracy': 0.853}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_1', 'Accuracy': 0.996}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_2', 'Accuracy': 0.819}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_domain_3', 'Accuracy': 0.632}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'principle_A_reconstruction', 'Accuracy': 0.516}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_1', 'Accuracy': 0.867}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'regular_plural_subject_verb_agreement_2', 'Accuracy': 0.815}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_licensor_present', 'Accuracy': 0.967}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_negation_npi_scope', 'Accuracy': 0.571}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'sentential_subject_island', 'Accuracy': 0.367}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_1', 'Accuracy': 0.849}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'superlative_quantifiers_2', 'Accuracy': 0.775}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_1', 'Accuracy': 0.786}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'tough_vs_raising_2', 'Accuracy': 0.754}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'transitive', 'Accuracy': 0.725}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_island', 'Accuracy': 0.603}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_object_gap', 'Accuracy': 0.868}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap', 'Accuracy': 0.928}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_questions_subject_gap_long_distance', 'Accuracy': 0.861}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap', 'Accuracy': 0.96}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_no_gap_long_distance', 'Accuracy': 0.95}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap', 'Accuracy': 0.321}, {'Model': 'tokyotech-llm/Llama-3-Swallow-8B-v0.1', 'Task': 'wh_vs_that_with_gap_long_distance', 'Accuracy': 0.315}]
overall_accuracy_shared_translation_pairs:                                    Model   OVERALL
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.778164
overall_accuracy_shared_non_translation:                                    Model   OVERALL
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.527582
overall_accuracy_comp:                                    Model   OVERALL
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.785597
overall_accuracy_comp_L1_or_L2:                                    Model  OVERALL
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.75997
overall_accuracy_comp_L1_specific:                                    Model   OVERALL
0  tokyotech-llm/Llama-3-Swallow-8B-v0.1  0.759701
============================ META INFO ============================
L2: ja
intervention num: 5911
intervention_num percentage: 1.0 %.
THRESHOLD: 0
count_shared_ONLY(same semantics): 5911
completed. saved to csv.

=================================== RESOURCE INFORMATION ===================================
 Requested Resource:
 mem=256000000kb,walltime=168:00:00,ncpus=26,place=pack 
 Used Resource: 
 mem=2658332kb,walltime=52:47:24,ncpus=26,cpupercent=100,vmem=49484980kb
============================================================================================
